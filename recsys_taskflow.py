"""
recsys_taskflow.py

ä¸€ä¸ªçœŸå®å¯è¿è¡Œçš„â€œæ¨èç³»ç»Ÿè®­ç»ƒä»»åŠ¡è°ƒåº¦ä¸æ‰§è¡Œâ€ç¤ºä¾‹ã€‚

åŠŸèƒ½ï¼š
- æ¯å¤©å‡Œæ™¨ 2:00 è‡ªåŠ¨å‘å¸ƒæ¨¡å‹è®­ç»ƒä»»åŠ¡ï¼›
- Producer å°†ä»»åŠ¡å‘é€åˆ° Kafkaï¼›
- Consumer å¼‚æ­¥æ¶ˆè´¹ä»»åŠ¡ï¼›
- Executor æ‰§è¡Œæ¨èæ¨¡å‹è®­ç»ƒï¼›
- Redis å­˜å‚¨ä»»åŠ¡çŠ¶æ€ï¼›
- Scheduler æ§åˆ¶ä»»åŠ¡è°ƒåº¦æ—¶é—´ã€‚

ä¾èµ–ï¼š
pip install kafka-python redis apscheduler scikit-learn pandas numpy
"""

import threading
import time
import json
import random
from datetime import datetime

import pandas as pd
import numpy as np
from sklearn.decomposition import TruncatedSVD

from apscheduler.schedulers.background import BackgroundScheduler
from kafka import KafkaProducer, KafkaConsumer
import redis


# ====== å…¨å±€é…ç½® ======
KAFKA_TOPIC = "recsys_task"
KAFKA_BOOTSTRAP = "localhost:9092"
REDIS_HOST = "localhost"
REDIS_PORT = 6379

rds = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, decode_responses=True)

producer = KafkaProducer(
    bootstrap_servers=KAFKA_BOOTSTRAP,
    value_serializer=lambda v: json.dumps(v).encode("utf-8"),
)

consumer = KafkaConsumer(
    KAFKA_TOPIC,
    bootstrap_servers=KAFKA_BOOTSTRAP,
    auto_offset_reset="earliest",
    enable_auto_commit=True,
    group_id="recsys_consumers",
    value_deserializer=lambda v: json.loads(v.decode("utf-8")),
)


# ====== æ¨¡å‹å®šä¹‰ ======
class RecsysModel:
    """
    ä¸€ä¸ªç®€å•çš„çŸ©é˜µåˆ†è§£æ¨èæ¨¡å‹ (TruncatedSVD å®ç°)ã€‚

    ç”¨äºæ¨¡æ‹Ÿæ¨èç³»ç»Ÿè®­ç»ƒä¸æ¨èé˜¶æ®µçš„è¿‡ç¨‹ã€‚

    Attributes:
        n_components (int): æ½œåœ¨å› å­ç»´åº¦ã€‚
        model (TruncatedSVD): sklearn æ¨¡å‹å®ä¾‹ã€‚
        user_index (Index): ç”¨æˆ·ç´¢å¼•ã€‚
        item_index (Index): ç‰©å“ç´¢å¼•ã€‚
        latent (ndarray): ç”¨æˆ·æ½œåœ¨å‘é‡çŸ©é˜µã€‚
    """

    def __init__(self, n_components: int = 10):
        """
        åˆå§‹åŒ–æ¨¡å‹ã€‚

        Args:
            n_components (int): æ½œåœ¨å› å­ç»´åº¦ã€‚
        """
        self.n_components = n_components
        self.model = TruncatedSVD(n_components=n_components)

    def train(self, df: pd.DataFrame):
        """
        è®­ç»ƒçŸ©é˜µåˆ†è§£æ¨¡å‹ã€‚

        Args:
            df (pd.DataFrame): åŒ…å«åˆ— ["user", "item", "rating"] çš„è¯„åˆ†æ•°æ®ã€‚

        Returns:
            RecsysModel: è¿”å›è‡ªèº«å¯¹è±¡ã€‚

        Example:
            >>> df = pd.DataFrame({"user":[0,0,1], "item":[1,2,2], "rating":[5,3,4]})
            >>> model = RecsysModel().train(df)
        """
        pivot = df.pivot(index="user", columns="item", values="rating").fillna(0)
        self.user_index = pivot.index
        self.item_index = pivot.columns
        self.latent = self.model.fit_transform(pivot)
        return self

    def recommend(self, user_id: int, top_k: int = 5):
        """
        æ ¹æ®è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œä¸ºæŒ‡å®šç”¨æˆ·æ¨èç‰©å“ã€‚

        Args:
            user_id (int): ç”¨æˆ· IDã€‚
            top_k (int): æ¨èç‰©å“æ•°é‡ã€‚

        Returns:
            list[int]: æ¨èç‰©å“ ID åˆ—è¡¨ã€‚

        Example:
            >>> model.recommend(12, top_k=3)
            [102, 59, 218]
        """
        if user_id not in self.user_index:
            return []
        user_vec = self.latent[self.user_index.get_loc(user_id)]
        item_latent = self.model.components_.T
        scores = np.dot(item_latent, user_vec)
        top_items = self.item_index[np.argsort(scores)[::-1][:top_k]]
        return list(map(int, top_items))


def generate_fake_data(n_users: int = 2000, n_items: int = 1000, seed: int = 42) -> pd.DataFrame:
    """
    ç”Ÿæˆæ¨¡æ‹Ÿçš„ user-item-rating æ•°æ®é›†ã€‚

    Args:
        n_users (int): ç”¨æˆ·æ•°ã€‚
        n_items (int): ç‰©å“æ•°ã€‚
        seed (int): éšæœºç§å­ã€‚

    Returns:
        pd.DataFrame: åŒ…å« user, item, rating ä¸‰åˆ—çš„æ•°æ®ã€‚

    Example:
        >>> df = generate_fake_data(3, 5)
        >>> df.head(2)
           user  item  rating
        0     1     4       3
        1     2     0       5
    """
    np.random.seed(seed)
    users = np.random.randint(0, n_users, size=100000)
    items = np.random.randint(0, n_items, size=100000)
    ratings = np.random.randint(1, 6, size=100000)
    return pd.DataFrame({"user": users, "item": items, "rating": ratings})


# ====== Producer ======
def publish_task(task_type: str = "train"):
    """
    å‘ Kafka å‘å¸ƒæ–°çš„è®­ç»ƒä»»åŠ¡ã€‚

    Args:
        task_type (str): ä»»åŠ¡ç±»å‹ï¼Œä¾‹å¦‚ "train"ã€"evaluate"ã€‚

    Example:
        >>> publish_task("train")
        [Producer] ğŸŸ¢ Published train task: train_1730512800
    """
    task_id = f"{task_type}_{int(time.time())}"
    task = {
        "id": task_id,
        "type": task_type,
        "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
    }

    # é¿å…é‡å¤ä»»åŠ¡
    if not rds.get(f"lock:{task_id}"):
        rds.set(f"lock:{task_id}", "1", ex=300)
        producer.send(KAFKA_TOPIC, value=task)
        producer.flush()
        rds.hset("tasks", task_id, "published")
        print(f"[Producer] ğŸŸ¢ Published {task_type} task: {task_id}")
    else:
        print(f"[Producer] â¸ Skipping duplicate task: {task_id}")


# ====== Executor ======
def execute_task(task: dict):
    """
    æ‰§è¡Œè®­ç»ƒä»»åŠ¡ï¼ˆæ¨¡æ‹Ÿè®­ç»ƒæ¨èæ¨¡å‹ï¼‰ã€‚

    Args:
        task (dict): ä»»åŠ¡å­—å…¸ï¼ŒåŒ…å« id, type, created_atã€‚

    Example:
        >>> execute_task({"id":"train_1730512800","type":"train"})
        [Executor] ğŸš€ Running task train_1730512800
        [Executor] âœ… Completed train_1730512800 (took 3.25s)
    """
    task_id = task["id"]
    print(f"[Executor] ğŸš€ Running task {task_id}")
    start_time = time.time()

    try:
        df = generate_fake_data()
        model = RecsysModel().train(df)
        user_id = random.choice(df["user"].unique())
        rec_items = model.recommend(user_id)

        elapsed = round(time.time() - start_time, 2)
        result = {
            "status": "success",
            "duration": elapsed,
            "user": int(user_id),
            "rec_items": rec_items[:5],
        }

        rds.hset("tasks", task_id, json.dumps(result))
        print(f"[Executor] âœ… Completed {task_id} (took {elapsed}s)")

    except Exception as e:
        rds.hset("tasks", task_id, json.dumps({"status": "error", "msg": str(e)}))
        print(f"[Executor] âŒ Error in {task_id}: {e}")


# ====== Consumer ======
def consume_tasks():
    """
    æŒç»­æ¶ˆè´¹ Kafka ä¸­çš„ä»»åŠ¡å¹¶è°ƒç”¨ Executor æ‰§è¡Œã€‚

    Example:
        >>> consume_tasks()
        [Consumer] ğŸŸ¦ Waiting for tasks ...
    """
    print("[Consumer] ğŸŸ¦ Waiting for tasks ...")
    for msg in consumer:
        task = msg.value
        execute_task(task)


# ====== Scheduler ======
def start_scheduler():
    """
    å¯åŠ¨ APSchedulerï¼Œæ¯å¤©å‡Œæ™¨ 2:00 è§¦å‘è®­ç»ƒä»»åŠ¡å‘å¸ƒã€‚

    Example:
        >>> start_scheduler()
        [Scheduler] â° Scheduler started (daily at 2:00 AM)
    """
    scheduler = BackgroundScheduler()
    scheduler.add_job(publish_task, "cron", hour=2, minute=0, kwargs={"task_type": "train"})
    scheduler.start()
    print("[Scheduler] â° Scheduler started (daily at 2:00 AM)")


# ====== ä¸»ç¨‹åºå…¥å£ ======
if __name__ == "__main__":
    print("ğŸ”¥ Starting RecSys TaskFlow Engine")

    # å¯åŠ¨ Scheduler
    threading.Thread(target=start_scheduler, daemon=True).start()

    # å¯åŠ¨ Consumer
    threading.Thread(target=consume_tasks, daemon=True).start()

    # ä¸»çº¿ç¨‹ç›‘æ§ä»»åŠ¡çŠ¶æ€
    try:
        while True:
            time.sleep(30)
            print("\n[Monitor] Redis task snapshot:")
            tasks = rds.hgetall("tasks")
            for k, v in list(tasks.items())[-5:]:
                print(f"  {k}: {v}")
            print("-" * 60)
    except KeyboardInterrupt:
        print("\nğŸ›‘ Shutting down ...")
